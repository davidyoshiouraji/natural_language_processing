{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbc5c67",
   "metadata": {},
   "source": [
    "Installation of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bertopic sec_edgar_downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc1153",
   "metadata": {},
   "source": [
    "Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sec_edgar_downloader import Downloader\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "def make_output_folder(folder_name):\n",
    "    # Make sure output folder exists and create if not\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return folder_name\n",
    "\n",
    "\n",
    "def download_filings(downloader, tickers, limit=6):\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            downloader.get(\"10-K\", ticker, limit=limit)\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"{ticker}: Failed to download. Error: {e}\")\n",
    "\n",
    "\n",
    "def collect_and_rename_files(tickers, output_folder):\n",
    "    # Search through downnloaded folder structure to collect and rename 10-K filings\n",
    "    for ticker in tickers:\n",
    "        base = os.path.join(\"sec-edgar-filings\", ticker, \"10-K\")\n",
    "        if not os.path.exists(base):\n",
    "            print(f\"{ticker}: Folder missing.\")\n",
    "            continue\n",
    "\n",
    "        for accession in sorted(os.listdir(base)):\n",
    "            source = os.path.join(base, accession, \"full-submission.txt\")\n",
    "            if not os.path.exists(source):\n",
    "                print(f\"{ticker}: Missing file {accession}\")\n",
    "                continue\n",
    "\n",
    "            # Try to get year from accession number\n",
    "            parts = accession.split(\"-\")\n",
    "            year = f\"20{parts[1]}\" if len(parts) > 1 else \"unknown\"\n",
    "\n",
    "            filename = f\"10K_{ticker}_{year}.txt\"\n",
    "            shutil.copy(source, os.path.join(output_folder, filename))\n",
    "            print(f\"{ticker}: Saved {filename}\")\n",
    "            \n",
    "            \n",
    "def filter_for_years():\n",
    "    input_folder = \"raw_10ks\"\n",
    "    # Filter files in the input folder for specific years\n",
    "    years = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "    # Delete files not in the specified years\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if not any(year in filename for year in years):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {filename} as it does not match the specified years.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    tickers = [\n",
    "        # Tech\n",
    "        \"AAPL\", \"GOOGL\", \"MSFT\", \"NVDA\", \"ORCL\",\n",
    "        # Financials\n",
    "        \"BAC\",  \"GS\", \"JPM\", \"MS\", \"V\",\n",
    "        # Healthcare\n",
    "        \"JNJ\", \"LLY\", \"MRK\", \"PFE\", \"UNH\"\n",
    "    ]\n",
    "\n",
    "    output = make_output_folder(\"raw_10ks\")\n",
    "    dl = Downloader(\"Copenhagen Business School\", \"daur24ac@student.cbs.dk\")\n",
    "\n",
    "    download_filings(dl, tickers)\n",
    "    collect_and_rename_files(tickers, output)\n",
    "    filter_for_years()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd553f5",
   "metadata": {},
   "source": [
    "Extraction of visible text out of 10-K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Extracts table of contents (TOC) anchor links matching relevant section names\n",
    "def get_clean_toc_links(soup, max_links=50):\n",
    "    VALID_CHAPTERS = [\n",
    "        \"business\", \"risk factors\", \"unresolved staff comments\", \"cybersecurity\", \"properties\",\n",
    "        \"legal proceedings\", \"mine safety disclosures\", \"market for the registrant’s common stock\",\n",
    "        \"management’s discussion and analysis\", \"quantitative and qualitative disclosures\",\n",
    "        \"financial statements\", \"changes in and disagreements\", \"controls and procedures\",\n",
    "        \"other information\", \"directors\", \"executive compensation\", \"security ownership\",\n",
    "        \"certain relationships\", \"principal accountant fees\", \"exhibits\", \"form 10-k summary\", \"signatures\"\n",
    "    ]\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True)[:max_links]:\n",
    "        text = a.get_text(strip=True).lower()\n",
    "        href = a[\"href\"].strip().lstrip(\"#\")\n",
    "        if any(valid in text for valid in VALID_CHAPTERS):\n",
    "            links.append((text, href))\n",
    "    return links\n",
    "\n",
    "\n",
    "# Finds the last HTML tag with the given ID (in case there are duplicates)\n",
    "def find_last_tag_by_id(soup, anchor_id):\n",
    "    matches = soup.find_all(id=anchor_id)\n",
    "    return matches[-1] if matches else None\n",
    "\n",
    "\n",
    "# Extracts clean visible text between two HTML tags\n",
    "def extract_visible_section_text(soup, start_tag, stop_tag=None):\n",
    "    buffer = []\n",
    "    for tag in start_tag.find_all_next():\n",
    "        if stop_tag and tag == stop_tag:\n",
    "            break\n",
    "        if tag.name in [\"script\", \"style\", \"head\", \"footer\", \"nav\", \"table\", \"form\"]:\n",
    "            continue\n",
    "        if tag.name in [\"div\", \"span\", \"section\", \"article\", \"p\", \"li\"]:\n",
    "            text = tag.get_text(strip=True, separator=\" \")\n",
    "            if text:\n",
    "                buffer.append(text)\n",
    "    return \"\\n\\n\".join(buffer).strip()\n",
    "\n",
    "\n",
    "# Simplifies a paragraph for deduplication\n",
    "def normalize_paragraph(p):\n",
    "    p = unicodedata.normalize(\"NFKC\", p)\n",
    "    p = p.strip().lower()\n",
    "    p = re.sub(r\"[ \\xa0\\t]+\", \" \", p)\n",
    "    p = re.sub(r\"[®™†*]+\", \"\", p)\n",
    "    p = re.sub(r\"[^\\w\\s]\", \"\", p)\n",
    "    return p\n",
    "\n",
    "\n",
    "# Removes repeated paragraphs based on normalized form\n",
    "def remove_duplicate_paragraphs(text):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in paragraphs:\n",
    "        norm = normalize_paragraph(p)\n",
    "        if norm and norm not in seen:\n",
    "            seen.add(norm)\n",
    "            unique.append(p.strip())\n",
    "    return \"\\n\\n\".join(unique).strip()\n",
    "\n",
    "\n",
    "# Pipeline to process 10-K filings\n",
    "def process_10k_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    toc_links = get_clean_toc_links(soup)\n",
    "\n",
    "    doc_sections = {}\n",
    "    for i, (label, anchor_id) in enumerate(toc_links):\n",
    "        start_tag = find_last_tag_by_id(soup, anchor_id)\n",
    "        if not start_tag:\n",
    "            continue\n",
    "        stop_tag = find_last_tag_by_id(soup, toc_links[i + 1][1]) if i + 1 < len(toc_links) else None\n",
    "        raw = extract_visible_section_text(soup, start_tag, stop_tag)\n",
    "        clean = remove_duplicate_paragraphs(raw)\n",
    "        doc_sections[label.lower()] = clean\n",
    "\n",
    "    return doc_sections\n",
    "\n",
    "\n",
    "# Set input/output folders\n",
    "INPUT_FOLDER = \"raw_10ks\"\n",
    "OUTPUT_FOLDER = \"processed_10ks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "# Process all files\n",
    "for filename in sorted(os.listdir(INPUT_FOLDER)):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_FOLDER, filename)\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, filename.replace(\".txt\", \".json\"))\n",
    "\n",
    "    print(f\"Processing {filename}...\")\n",
    "    cleaned_sections = process_10k_file(input_path)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(cleaned_sections, out, indent=2)\n",
    "\n",
    "print(\"All files processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef284ddc",
   "metadata": {},
   "source": [
    "Topic modelling of risk sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up constants\n",
    "SECTION_KEY = \"risk factors\"\n",
    "OUTPUT_TOPICS = \"risk_topics.csv\"\n",
    "INDUSTRY_MAP = {\n",
    "    \"AAPL\": \"Technology\", \"MSFT\": \"Technology\", \"GOOGL\": \"Technology\",\n",
    "    \"NVDA\": \"Technology\", \"ORCL\": \"Technology\",\n",
    "    \"JPM\": \"Financials\", \"BAC\": \"Financials\", \"GS\": \"Financials\",\n",
    "    \"WFC\": \"Financials\", \"MS\": \"Financials\",\n",
    "    \"JNJ\": \"Healthcare\", \"PFE\": \"Healthcare\", \"MRK\": \"Healthcare\",\n",
    "    \"LLY\": \"Healthcare\", \"UNH\": \"Healthcare\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_paragraphs(text, min_words=5):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    cleaned = []\n",
    "    for p in paragraphs:\n",
    "        p = p.strip()\n",
    "        if len(p.split()) < min_words:\n",
    "            continue\n",
    "        if re.search(r\"Form\\s+10-K|\\bItem\\b|\\bTable of Contents\\b|\\d{4}|\\bRisk Factors\\b\", p, re.IGNORECASE):\n",
    "            if len(p.split()) < 10:\n",
    "                continue\n",
    "        cleaned.append(p)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def load_clean_paragraphs(folder, section_key, industry_map):\n",
    "    rows = []\n",
    "    print(\"Loading and cleaning paragraphs...\")\n",
    "    for filename in os.listdir(folder):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "        parts = filename.replace(\".json\", \"\").split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        firm, year = parts[1], parts[2]\n",
    "        industry = industry_map.get(firm)\n",
    "        if not industry:\n",
    "            continue\n",
    "        with open(os.path.join(folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        section_text = data.get(section_key, \"\")\n",
    "        if section_text:\n",
    "            for p in extract_paragraphs(section_text):\n",
    "                rows.append({\n",
    "                    \"firm\": firm,\n",
    "                    \"year\": int(year),\n",
    "                    \"industry\": industry,\n",
    "                    \"text\": p.strip()\n",
    "                })\n",
    "    print(f\"Extracted {len(rows)} clean paragraphs.\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def topic_model(input_df, top_n_keywords=3):\n",
    "    print(\"Running BERTopic on\", len(input_df), \"paragraphs...\")\n",
    "    embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "    model = BERTopic(embedding_model=embedding_model, calculate_probabilities=False, verbose=True)\n",
    "\n",
    "    topics, _ = model.fit_transform(input_df[\"text\"].tolist())\n",
    "    input_df[\"topic\"] = topics\n",
    "\n",
    "    topic_labels = {\n",
    "        topic: \", \".join([kw for kw, _ in model.get_topic(topic)[:top_n_keywords]])\n",
    "        if topic != -1 else \"Other\"\n",
    "        for topic in model.get_topic_freq().Topic\n",
    "    }\n",
    "\n",
    "    input_df[\"topic_label\"] = input_df[\"topic\"].map(topic_labels)\n",
    "    input_df.to_csv(OUTPUT_TOPICS, index=False)\n",
    "    print(\"Saved:\", OUTPUT_TOPICS)\n",
    "    return input_df, topic_labels\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = load_clean_paragraphs(OUTPUT_FOLDER, SECTION_KEY, INDUSTRY_MAP)\n",
    "    df = df.drop_duplicates(subset=[\"firm\", \"year\", \"text\"])\n",
    "    print(\"Paragraphs cleaned and ready for topic modeling.\")\n",
    "    df_with_topics, topic_labels = topic_model(df)\n",
    "    return df_with_topics, topic_labels\n",
    "\n",
    "\n",
    "# Capture results for futher use\n",
    "df_with_topics, topic_labels = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaf118",
   "metadata": {},
   "source": [
    "Visulization of topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "def find_top_topics(df, top_n=10):\n",
    "    df = df[df[\"topic_label\"] != \"Other\"]\n",
    "    top_overall = (\n",
    "        df[\"topic_label\"]\n",
    "        .value_counts()\n",
    "        .head(top_n)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"topic_label\", \"topic_label\": \"count\"})\n",
    "    )\n",
    "    top_overall.to_csv(\"top_topics_total.csv\", index=False)\n",
    "\n",
    "\n",
    "def plot_top_overall(df, top_n=10):\n",
    "    df = df[df[\"topic_label\"] != \"Other\"]\n",
    "    top_topics = df[\"topic_label\"].value_counts().head(top_n)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=top_topics.values, y=top_topics.index, palette=\"Blues_d\")\n",
    "    plt.xlabel(\"Paragraph Count\")\n",
    "    plt.ylabel(\"Topic Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_10_most_frequent_risk_topics.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def top_topics_per_industry(df, top_n=5):\n",
    "    df = df[df[\"topic_label\"] != \"Other\"]\n",
    "    grouped = (\n",
    "        df.groupby([\"industry\", \"topic_label\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values([\"industry\", \"count\"], ascending=[True, False])\n",
    "    )\n",
    "    top_per_industry = grouped.groupby(\"industry\").head(top_n)\n",
    "    top_per_industry.to_csv(\"top_topics_per_industry.csv\", index=False)\n",
    "\n",
    "    for industry, filename in zip(\n",
    "        [\"Financials\", \"Healthcare\", \"Technology\"],\n",
    "        [\n",
    "            \"top_5_risk_topics_financials.png\",\n",
    "            \"top_5_risk_topics_healthcare.png\",\n",
    "            \"top_5_risk_topics_technology.png\"\n",
    "        ]\n",
    "    ):\n",
    "        subset = top_per_industry[top_per_industry[\"industry\"] == industry]\n",
    "        if not subset.empty:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.barplot(x=\"count\", y=\"topic_label\", data=subset, palette=\"Set2\")\n",
    "            plt.xlabel(\"Paragraph Count\")\n",
    "            plt.ylabel(\"Topic\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(filename)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def plot_industry_risk_profile(df, min_topic_count=50):\n",
    "    counts = df.groupby([\"industry\", \"topic_label\"]).size().unstack(fill_value=0)\n",
    "    counts = counts.loc[:, counts.sum() > min_topic_count]\n",
    "    counts_norm = counts.div(counts.sum(axis=1), axis=0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(counts_norm.T, cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\n",
    "    plt.xlabel(\"Industry\")\n",
    "    plt.ylabel(\"Topic\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"relative_emphasis_on_risk_topics.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_topic_trends_over_time(df, top_n=10):\n",
    "    df = df[df[\"topic_label\"] != \"Other\"]\n",
    "    top_labels = df[\"topic_label\"].value_counts().head(top_n).index\n",
    "    df_top = df[df[\"topic_label\"].isin(top_labels)]\n",
    "    pivot = df_top.groupby([\"year\", \"topic_label\"]).size().unstack(fill_value=0)\n",
    "    ax = pivot.plot(kind=\"line\", marker=\"o\", figsize=(10, 5))\n",
    "    ax.set_xticks(sorted(df[\"year\"].unique()))\n",
    "    ax.set_xticklabels(sorted(df[\"year\"].unique()), rotation=0)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Paragraph Count\")\n",
    "    plt.legend(title=\"Topic\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_10_risk_topics_over_time.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_topic_trends_by_industry(df, top_n=5):\n",
    "    df = df[df[\"topic_label\"] != \"Other\"]\n",
    "    for industry, filename in zip(\n",
    "        [\"Financials\", \"Healthcare\", \"Technology\"],\n",
    "        [\n",
    "            \"top_5_risk_topics_over_time_financials.png\",\n",
    "            \"top_5_risk_topics_over_time_healthcare.png\",\n",
    "            \"top_5_risk_topics_over_time_technology.png\"\n",
    "        ]\n",
    "    ):\n",
    "        df_ind = df[df[\"industry\"] == industry]\n",
    "        top_labels = df_ind[\"topic_label\"].value_counts().head(top_n).index\n",
    "        df_top = df_ind[df_ind[\"topic_label\"].isin(top_labels)]\n",
    "        pivot = df_top.groupby([\"year\", \"topic_label\"]).size().unstack(fill_value=0)\n",
    "        ax = pivot.plot(kind=\"line\", marker=\"o\", figsize=(10, 5), title=f\"{industry} – Top {top_n} Topics Over Time\")\n",
    "        ax.set_xticks(sorted(df[\"year\"].unique()))\n",
    "        ax.set_xticklabels(sorted(df[\"year\"].unique()), rotation=0)\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Paragraph Count\")\n",
    "        plt.legend(title=\"Topic\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_total_paragraph_volume(df):\n",
    "    yearly_counts = df.groupby(\"year\").size().reset_index(name=\"paragraph_count\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=\"year\", y=\"paragraph_count\", data=yearly_counts, palette=\"muted\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Paragraph Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"risk_paragraph_count.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_risk_section_length(df):\n",
    "    df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "    lengths = df.groupby([\"firm\", \"year\", \"industry\"])[\"word_count\"].sum().reset_index()\n",
    "    avg_by_industry = lengths.groupby([\"industry\", \"year\"])[\"word_count\"].mean().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=avg_by_industry, x=\"year\", y=\"word_count\", hue=\"industry\", marker=\"o\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Average Word Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"risk_section_length.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    find_top_topics(df_with_topics)\n",
    "    plot_top_overall(df_with_topics)\n",
    "    top_topics_per_industry(df_with_topics)\n",
    "    plot_industry_risk_profile(df_with_topics)\n",
    "    plot_topic_trends_over_time(df_with_topics, top_n=10)\n",
    "    plot_topic_trends_by_industry(df_with_topics, top_n=5)\n",
    "    plot_total_paragraph_volume(df_with_topics)\n",
    "    plot_risk_section_length(df_with_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_top_risk_disclosures(df, top_n=10):\n",
    "    # Calculate word count per company and year\n",
    "    df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "    totals = df.groupby([\"firm\", \"year\", \"industry\"])[\"word_count\"].sum().reset_index()\n",
    "    top_longest = totals.sort_values(\"word_count\", ascending=False).head(top_n)\n",
    "    return top_longest\n",
    "\n",
    "print(\"Top companies by total risk disclosure length:\")\n",
    "list_top_risk_disclosures(df_with_topics, top_n=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
